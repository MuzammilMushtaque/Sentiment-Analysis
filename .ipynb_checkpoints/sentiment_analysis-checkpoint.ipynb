{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d72bb7b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis <br>\n",
    "\n",
    "## Muzammil Mushtaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2877d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('restaurant-reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa56e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Initial Dataset before preprocessing :  (1000, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>restaurant_url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manufactur</td>\n",
       "      <td>https://www.tripadvisor.com/Restaurant_Review-...</td>\n",
       "      <td>Best in Kiel</td>\n",
       "      <td>The absolutely best restaurant in the town of ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Manufactur</td>\n",
       "      <td>https://www.tripadvisor.com/Restaurant_Review-...</td>\n",
       "      <td>Simply, tasty and very good</td>\n",
       "      <td>Tasty and high quality food! A “healthier”way ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name                                     restaurant_url  \\\n",
       "0  Manufactur  https://www.tripadvisor.com/Restaurant_Review-...   \n",
       "1  Manufactur  https://www.tripadvisor.com/Restaurant_Review-...   \n",
       "\n",
       "                         title  \\\n",
       "0                 Best in Kiel   \n",
       "1  Simply, tasty and very good   \n",
       "\n",
       "                                                text  rating  \n",
       "0  The absolutely best restaurant in the town of ...     5.0  \n",
       "1  Tasty and high quality food! A “healthier”way ...     5.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Shape of the Initial Dataset before preprocessing : ', df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b6ed44",
   "metadata": {},
   "source": [
    "### Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409fc514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing.\n",
    "df['text'] = [str(text.lower()) for text in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac86941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Non English Reviews (just for my understading :P )\n",
    "from langdetect import detect\n",
    "def detect_language(text):\n",
    "    \n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return language == 'en'\n",
    "    except:\n",
    "        return False  \n",
    "\n",
    "df = df[df['text'].apply(detect_language)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6897e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling special characters and Numbers\n",
    "import re\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^A-Za-z\\s]', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfe6235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatize_sentence(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in sentence.split()])\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68bd99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword Removal\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(sentence):\n",
    "    return ' '.join([word for word in sentence.split() if word.lower() not in stop_words])\n",
    "\n",
    "df['text'] = df['text'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373cb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary modification\n",
    "df['rating'] = df['rating'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "740a3b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset after preprocessing : (964, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>absolutely best restaurant town kiel nice deco...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tasty high quality food healthierway propose c...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food wa asked happily surprised food small res...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazing service amzing food nice people quite ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>manufaktur really nice small self service rest...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>traum gmbh traumfabrik former name kiel reside...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>great big place nice patio like fireplace staf...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>big choice many different dish tried wa good l...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>bad service</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>ordered delivery grill platten zeusplatte bift...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>964 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  rating\n",
       "0    absolutely best restaurant town kiel nice deco...     4.0\n",
       "1    tasty high quality food healthierway propose c...     4.0\n",
       "2    food wa asked happily surprised food small res...     4.0\n",
       "3    amazing service amzing food nice people quite ...     4.0\n",
       "4    manufaktur really nice small self service rest...     4.0\n",
       "..                                                 ...     ...\n",
       "995  traum gmbh traumfabrik former name kiel reside...     3.0\n",
       "996  great big place nice patio like fireplace staf...     1.0\n",
       "997  big choice many different dish tried wa good l...     3.0\n",
       "998                                        bad service     0.0\n",
       "999  ordered delivery grill platten zeusplatte bift...     3.0\n",
       "\n",
       "[964 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print ('Shape of dataset after preprocessing :', df.shape)\n",
    "df[['text', 'rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a433b8",
   "metadata": {},
   "source": [
    "#### Heuristic Approaches to Sentiment Analysis <br>\n",
    "\n",
    "TextBlob uses a heuristic approach for sentiment analysis, relying on predefined patterns and rules. It assigns polarity scores to words, calculates the overall sentiment of a sentence based on these scores, and provides subjectivity scores. It also includes a basic Naive Bayes classifier for sentiment analysis. While convenient, this rule-based approach may be less accurate than models trained on specific datasets. <br>\n",
    "##### *Importance of TextBlob*\n",
    "\n",
    "TextBlob's sentiment analysis uses a mix of machine learning and rules.\n",
    "\n",
    "1. **Preprocessing:** Clean and prepare the input text.\n",
    "\n",
    "2. **Text Parsing:** Break down the text into sentences and words.\n",
    "\n",
    "3. **Feature Extraction:** Extract important elements for sentiment analysis.\n",
    "\n",
    "4. **Machine Learning Model:** Use a pre-trained model for sentiment classification.\n",
    "\n",
    "5. **Rule-based Classification:** Apply predefined rules for linguistic nuances.\n",
    "\n",
    "6. **Sentiment Polarity and Subjectivity:** Calculate polarity (positive/negative/neutral) and subjectivity.\n",
    "\n",
    "7. **Final Sentiment Output:** Assign a sentiment label based on machine learning and rules.\n",
    "\n",
    "8. **Probability Scores:** Provide confidence scores for sentiment predictions.\n",
    "\n",
    "\n",
    "In the previous steps, although we have conducted the Text preprocessing mainly because of the learning and practice purpose. TextBlob has the predefined algorithm to clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df591536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>absolutely best restaurant town kiel nice deco...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tasty high quality food healthierway propose c...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food wa asked happily surprised food small res...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazing service amzing food nice people quite ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>manufaktur really nice small self service rest...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>traum gmbh traumfabrik former name kiel reside...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>great big place nice patio like fireplace staf...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>big choice many different dish tried wa good l...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>bad service</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>ordered delivery grill platten zeusplatte bift...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>964 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  sentiment_analysis\n",
       "0    absolutely best restaurant town kiel nice deco...                   4\n",
       "1    tasty high quality food healthierway propose c...                   3\n",
       "2    food wa asked happily surprised food small res...                   3\n",
       "3    amazing service amzing food nice people quite ...                   4\n",
       "4    manufaktur really nice small self service rest...                   3\n",
       "..                                                 ...                 ...\n",
       "995  traum gmbh traumfabrik former name kiel reside...                   3\n",
       "996  great big place nice patio like fireplace staf...                   4\n",
       "997  big choice many different dish tried wa good l...                   3\n",
       "998                                        bad service                   0\n",
       "999  ordered delivery grill platten zeusplatte bift...                   3\n",
       "\n",
       "[964 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def filter_reviews(reviews):\n",
    "    filtered_reviews = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        analysis = TextBlob(review)\n",
    "        sentiment_score = analysis.sentiment.polarity\n",
    "        if sentiment_score >= 0.5:\n",
    "            filtered_reviews.append(4)\n",
    "        elif 0 < sentiment_score < 0.5:\n",
    "            filtered_reviews.append(3)\n",
    "        elif sentiment_score == 0 :\n",
    "            filtered_reviews.append(2)\n",
    "        elif -0.5 < sentiment_score < 0.0:\n",
    "            filtered_reviews.append(1)\n",
    "        else:\n",
    "            filtered_reviews.append(0)\n",
    "    return filtered_reviews\n",
    "\n",
    "\n",
    "df['sentiment_analysis'] = filter_reviews(df['text'])\n",
    "\n",
    "df[['text','sentiment_analysis']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf6f103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.05      0.09        39\n",
      "         1.0       0.27      0.26      0.27        53\n",
      "         2.0       0.00      0.00      0.00       102\n",
      "         3.0       0.37      0.81      0.51       327\n",
      "         4.0       0.65      0.28      0.39       443\n",
      "\n",
      "    accuracy                           0.42       964\n",
      "   macro avg       0.36      0.28      0.25       964\n",
      "weighted avg       0.46      0.42      0.37       964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report to find relation between actual rating and textblob rating\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(df['rating'], df['sentiment_analysis'])\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(df['rating'], df['sentiment_analysis']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4d021",
   "metadata": {},
   "source": [
    "#### Finetuned Transformer Model for Sentiment Analysis\n",
    "\n",
    "Fine-tuning a transformer model for sentiment analysis involves taking a pre-trained transformer model (such as BERT, GPT, or others) and training it on a dataset specifically designed for sentiment analysis tasks.\n",
    "\n",
    "#### Purpose & Resources of Finetuned Transformer Model\n",
    "Fine-tuning a transformer model serves the purpose of adapting a pre-trained model on a specific downstream task or dataset. Transformer models, like BERT, GPT, or others, are typically pre-trained on large datasets and learn general language representations. However, these pre-trained models may not be optimized for specific tasks, such as sentiment analysis, question answering, or named entity recognition.\n",
    "\n",
    "The main purposes of fine-tuning a transformer model are:\n",
    "\n",
    "1. **Task-Specific Adaptation:**\n",
    "   - Fine-tuning allows you to adapt a pre-trained model to your specific task or domain. For example, you can take a transformer model pre-trained on a general corpus and fine-tune it on a sentiment analysis dataset to make it specific to sentiment prediction.\n",
    "\n",
    "2. **Improved Performance on Specific Tasks:**\n",
    "   - Fine-tuning helps improve the model's performance on a specific task by leveraging the knowledge and representations learned during pre-training. The model can capture task-specific nuances and patterns during the fine-tuning process.\n",
    "\n",
    "3. **Reduced Training Time and Resources:**\n",
    "   - Training a transformer model from scratch requires substantial computational resources and time. Fine-tuning is more computationally efficient since it leverages the knowledge already present in a pre-trained model, saving both time and resources.\n",
    "\n",
    "4. **Transfer Learning:**\n",
    "   - Fine-tuning enables transfer learning, where knowledge gained from a source task (pre-training) is transferred to a target task (fine-tuning). This is especially useful when the target task has limited labeled data.\n",
    "\n",
    "Here's a simplified process for fine-tuning a transformer model:\n",
    "\n",
    "- **Pre-training:** Train a transformer model on a large corpus with a self-supervised objective (e.g., predicting missing words in a sentence).\n",
    "\n",
    "- **Fine-tuning:** Further train the pre-trained model on a smaller dataset related to your specific task (e.g., sentiment analysis) with labeled examples. This fine-tuning process updates the model's parameters to be more task-specific.\n",
    "\n",
    "The fine-tuned model can then be used for making predictions on new data in the target domain or task.\n",
    "\n",
    "In summary, fine-tuning allows you to take advantage of pre-trained models' general language understanding and adapt them to specific tasks, improving performance and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "101634fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe after removing rating 0 : (924, 6)\n"
     ]
    }
   ],
   "source": [
    "'''                                 We are ignoring the rating 1, the only \n",
    "                                    reason is to increase computational time\n",
    "                                    by generating small random + balance dataset as \n",
    "                                    we have done in later part.\n",
    "'''\n",
    "#df = df[df['rating'] != 0.0].copy()\n",
    "#print ('Shape of dataframe after removing rating 0 :', df.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bafbc",
   "metadata": {},
   "source": [
    "#### BERT Finetuned Transformer Model for Sentiment Analysis: <br>\n",
    "\n",
    "The key points of importance of BERT are, <br>\n",
    "\n",
    "*Contextual Understanding:* BERT captures bidirectional context for better understanding.\n",
    "\n",
    "*Pre-training on Large Corpora:* Pre-trained on large datasets for rich language representations.\n",
    "\n",
    "*Transfer Learning:* Transfers pre-learned knowledge to specific tasks.\n",
    "\n",
    "*Fine-tuning for Specific Tasks:* Adapts pre-trained BERT to task-specific nuances.\n",
    "\n",
    "*Handling Complex Sentiments:* Effectively handles complex and nuanced sentiments.\n",
    "\n",
    "*Out-of-the-Box Performance:* Strong performance without extensive feature engineering.\n",
    "\n",
    "*Handling Polysemy and Ambiguity:* Deals well with multiple word meanings and ambiguity.\n",
    "\n",
    "*State-of-the-Art Results:* Achieves state-of-the-art results on NLP benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7fee080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Randomly Selected Dataset :  (121, 6)\n",
      "Unique set of Labels in our training/testing dataset {0.0, 1.0, 2.0, 3.0, 4.0} {0.0, 1.0, 2.0, 3.0, 4.0}\n"
     ]
    }
   ],
   "source": [
    "'''                            Only 12% dataset has been Random selected for Training Datset\n",
    "                               And Test Dataset for the quick analysis. \n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "total_df = df.sample(frac=0.1255, random_state=None) \n",
    "print('Shape of the Randomly Selected Dataset : ',total_df.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_df['text'], total_df['rating'], test_size=0.8, random_state=None)\n",
    "print('Unique set of Labels in our training/testing dataset',set(y_train.values), set(y_test.values))\n",
    "#print(set(y_test.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a4dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a8d0fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\AK traders\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_bert_sentiment_model\\\\tokenizer_config.json',\n",
       " 'fine_tuned_bert_sentiment_model\\\\special_tokens_map.json',\n",
       " 'fine_tuned_bert_sentiment_model\\\\vocab.txt',\n",
       " 'fine_tuned_bert_sentiment_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''                             Trained the BERT Model for Sentiment Analysis\n",
    "\n",
    "'''\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)  \n",
    "\n",
    "# Tokenize and encode the dataset\n",
    "texts = X_train.tolist()\n",
    "labels = torch.tensor(y_train.values, dtype=torch.long)\n",
    "#labels = torch.clamp(labels - 1, min=0)\n",
    "#print (labels)\n",
    "encoded_texts = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Create a DataLoader for training\n",
    "dataset = TensorDataset(encoded_texts['input_ids'], encoded_texts['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Set up optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tune the model\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, label = batch\n",
    "       # print (label)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=label)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_bert_sentiment_model')\n",
    "tokenizer.save_pretrained('fine_tuned_bert_sentiment_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb59b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('fine_tuned_bert_sentiment_model')\n",
    "model = BertForSequenceClassification.from_pretrained('fine_tuned_bert_sentiment_model')\n",
    "\n",
    "#def test_BERT(text_to_predict):\n",
    "# Text to predict sentiment for\n",
    "text_to_predict = X_test.tolist()\n",
    "# Tokenize and encode the text\n",
    "encoded_text = tokenizer(text_to_predict, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(**encoded_text)\n",
    "\n",
    "# Access the logits (raw scores before softmax) for sentiment prediction\n",
    "logits = outputs.logits\n",
    "\n",
    "# Perform softmax to get probabilities\n",
    "probabilities = logits.softmax(dim=-1)\n",
    "\n",
    "predicted_label = torch.argmax(probabilities,dim=-1)\n",
    "predicted_label = predicted_label.tolist()\n",
    "\n",
    "predicted_sentiment = predicted_label\n",
    "#print (y_test, predicted_sentiment)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predicted_sentiment)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, predicted_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5ca22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636    1.0\n",
      "288    4.0\n",
      "116    4.0\n",
      "435    3.0\n",
      "430    4.0\n",
      "      ... \n",
      "944    1.0\n",
      "489    3.0\n",
      "26     4.0\n",
      "439    4.0\n",
      "585    4.0\n",
      "Name: rating, Length: 97, dtype: float64 [3, 3, 4, 4, 4, 4, 4, 3, 4, 3, 3, 4, 3, 4, 3, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3]\n",
      "Accuracy: 0.44\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00         2\n",
      "         1.0       0.00      0.00      0.00         5\n",
      "         2.0       0.00      0.00      0.00        10\n",
      "         3.0       0.40      0.69      0.51        39\n",
      "         4.0       0.53      0.39      0.45        41\n",
      "\n",
      "    accuracy                           0.44        97\n",
      "   macro avg       0.19      0.22      0.19        97\n",
      "weighted avg       0.39      0.44      0.40        97\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904a922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5215878",
   "metadata": {},
   "source": [
    "#### Train a Model <br>\n",
    "##### *Train a Multinomial Naive Bayes (NB) machine learning model based on TF-IDF* <br>\n",
    "\n",
    "**Algorithm: Training Multinomial Naive Bayes (NB) Model based on TF-IDF:**\n",
    "\n",
    "1. **Text Preprocessing:**\n",
    "   - Tokenize and preprocess the text data, including steps like lowercasing, removing stop words, and stemming if needed.\n",
    "\n",
    "2. **TF-IDF Vectorization:**\n",
    "   - Convert the text data into a numerical format using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization.\n",
    "   - Calculate the TF-IDF scores for each term in the documents.\n",
    "\n",
    "3. **Labeling:**\n",
    "   - Assign labels to the documents based on the corresponding categories or classes.\n",
    "\n",
    "4. **Multinomial Naive Bayes Training:**\n",
    "   - Train the Multinomial Naive Bayes model using the TF-IDF vectors and the assigned labels.\n",
    "   - The model estimates the probabilities of each term's occurrence given the class and uses Bayes' theorem to make predictions.\n",
    "\n",
    "5. **Model Evaluation (Optional):**\n",
    "   - Evaluate the trained model on a separate validation set or through cross-validation to assess its performance.\n",
    "\n",
    "6. **Inference (Prediction):**\n",
    "   - Use the trained model to predict the class or category of new, unseen text data.\n",
    "\n",
    "**Benefits of Multinomial Naive Bayes based on TF-IDF:**\n",
    "\n",
    "1. **Efficiency:**\n",
    "   - Computationally efficient and fast, making it suitable for large datasets and real-time applications.\n",
    "\n",
    "2. **Simple and Interpretable:**\n",
    "   - Easy to implement and interpret, with a clear probabilistic framework.\n",
    "\n",
    "3. **Handles Multiclass Classification:**\n",
    "   - Well-suited for multiclass classification problems, where documents can belong to more than two categories.\n",
    "\n",
    "4. **Works well with Text Data:**\n",
    "   - Effective for text classification tasks, especially when dealing with a large number of features (words in a vocabulary).\n",
    "\n",
    "5. **Naive Independence Assumption:**\n",
    "   - The Naive Bayes assumption of feature independence simplifies the model and often performs well in practice.\n",
    "\n",
    "6. **TF-IDF for Feature Representation:**\n",
    "   - TF-IDF provides a meaningful representation of terms in documents, highlighting important words while downplaying common ones.\n",
    "\n",
    "7. **Adaptable to Streaming Data:**\n",
    "   - Naive Bayes models can be updated incrementally, making them adaptable to streaming or changing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b81a027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.63\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "         2.0       0.00      0.00      0.00         4\n",
      "         3.0       0.56      0.59      0.57        17\n",
      "         4.0       0.68      0.88      0.76        24\n",
      "\n",
      "    accuracy                           0.63        49\n",
      "   macro avg       0.31      0.37      0.33        49\n",
      "weighted avg       0.52      0.63      0.57        49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Train a suitable machine learning model based on TF-IDF\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['rating'], test_size=0.05, random_state=None)\n",
    "\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a machine learning model (Naive Bayes in this example)\n",
    "# Add alpha as a hyperparameter\n",
    "alpha = 0.1  \n",
    "classifier = MultinomialNB(alpha=alpha)\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a670ce6",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " > Use scikit-learn's TfidfVectorizer to convert the raw text into a TF-IDF feature representation.\n",
    "\n",
    " > The Multinomial Naive Bayes is trained on the TF-IDF transformed training data.\n",
    "\n",
    " > Predictions are made on the test set.\n",
    "\n",
    " > The model is evaluated using accuracy and a classification report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf89b15d",
   "metadata": {},
   "source": [
    "##### *Train a Logistic Regression Machine Learning Model based on Word Embedding*\n",
    "\n",
    "**Training a Logistic Regression Model Based on Word Embedding:**\n",
    "\n",
    "1. **Word Embedding:**\n",
    "   - Preprocess text data and tokenize it into words.\n",
    "   - Convert words into dense vectors using word embedding techniques like Word2Vec, GloVe, or embeddings layers in deep learning models.\n",
    "\n",
    "2. **Feature Extraction:**\n",
    "   - For each document, obtain the word embeddings of individual words.\n",
    "   - Aggregate word embeddings to obtain a fixed-size feature vector for each document.\n",
    "\n",
    "3. **Model Training:**\n",
    "   - Use the aggregated word embeddings as input features for logistic regression.\n",
    "   - Train a logistic regression model to predict the binary class labels based on the feature vectors.\n",
    "\n",
    "4. **Prediction:**\n",
    "   - Given a new document, tokenize and convert it into word embeddings.\n",
    "   - Aggregate the word embeddings and use the trained logistic regression model to predict the class label.\n",
    "\n",
    "**Benefits of Logistic Regression with Word Embedding:**\n",
    "\n",
    "1. **Captures Semantic Information:**\n",
    "   - Word embeddings capture semantic relationships between words, allowing the model to understand the meaning of words in context.\n",
    "\n",
    "2. **Variable-Length Input Handling:**\n",
    "   - Word embeddings enable handling variable-length documents by converting them into fixed-size feature vectors.\n",
    "\n",
    "3. **Interpretability:**\n",
    "   - Logistic regression provides interpretable results by assigning weights to features (word embeddings).\n",
    "   - It allows understanding which words contribute more or less to the prediction.\n",
    "\n",
    "4. **Efficient for Linear Separation:**\n",
    "   - Logistic regression is effective when the relationship between features and the target is approximately linear.\n",
    "   - It can perform well in scenarios where the decision boundary is relatively simple.\n",
    "\n",
    "5. **Scalability:**\n",
    "   - Word embeddings can be pre-trained on large corpora and then fine-tuned for specific tasks.\n",
    "   - This leverages the benefits of transfer learning, especially when labeled data is limited.\n",
    "\n",
    "6. **Suitable for Binary Classification:**\n",
    "   - Logistic regression is a natural choice for binary classification problems, such as sentiment analysis or spam detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "908d947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.45\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "         2.0       0.00      0.00      0.00         4\n",
      "         3.0       0.00      0.00      0.00        17\n",
      "         4.0       0.47      0.92      0.62        24\n",
      "\n",
      "    accuracy                           0.45        49\n",
      "   macro avg       0.12      0.23      0.15        49\n",
      "weighted avg       0.23      0.45      0.30        49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Word embeddings are dense vector representations of words that capture semantic relationships between words. One popular method for generating word embeddings is Word2Vec. I will show how to train a sentiment analysis model using Word2Vec embeddings with Python and gensim.\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_train = [gensim.utils.simple_preprocess(text) for text in X_train]\n",
    "tokenized_test = [gensim.utils.simple_preprocess(text) for text in X_test]\n",
    "\n",
    "# Train Word2Vec model with hyperparameters vector_size=100 and window=5\n",
    "vector_size = 100\n",
    "window = 5\n",
    "model = Word2Vec(sentences=tokenized_train, vector_size=vector_size, window=window, min_count=1, workers=4)\n",
    "model.train(tokenized_train, total_examples=len(tokenized_train), epochs=10)\n",
    "\n",
    "\n",
    "# Function to get the vector representation of a sentence\n",
    "def get_sentence_vector(sentence):\n",
    "    vector_sum = np.zeros(model.vector_size)\n",
    "    for word in sentence:\n",
    "        if word in model.wv:\n",
    "            vector_sum += model.wv[word]\n",
    "    return vector_sum / len(sentence)\n",
    "\n",
    "# Convert sentences to vectors\n",
    "X_train_vectors = [get_sentence_vector(sentence) for sentence in tokenized_train]\n",
    "X_test_vectors = [get_sentence_vector(sentence) for sentence in tokenized_test]\n",
    "\n",
    "# Train a machine learning model (Logistic Regression in this example)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test_vectors)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1f538a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
